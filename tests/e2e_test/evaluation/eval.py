import os
from typing import List, Dict, Any
from ragas import evaluate
from ragas.metrics import (
    faithfulness,  # Measures if the answer is grounded in the context
    answer_relevancy,  # Measures if the answer is relevant to the question
    context_precision,  # Measures the signal-to-noise ratio in retrieved contexts
    context_recall  # Measures if all necessary information was retrieved
)
from datasets import Dataset
from langchain_openai import ChatOpenAI
import pandas as pd

llm_for_eval = ChatOpenAI(
    base_url=os.environ["OPENAI_API_BASE"],
    api_key=os.environ["OPENAI_API_KEY"],
    model="gpt-3.5-turbo",
    temperature=0
)


# --- Step 2: Define Your RAG System Interface ---
def my_rag_system(question: str) -> Dict[str, Any]:
    """
    Placeholder for your RAG system logic. Replace this with your actual implementation.

    Args:
        question: The input question string from the user.

    Returns:
        A dictionary containing:
            - 'answer' (str): The answer generated by your RAG system.
            - 'contexts' (List[str]): A list of context strings retrieved by your RAG system.
    """
    print(f"Executing RAG system for question: '{question}'")

    # ========== TODO: Implement your RAG system logic here ==========
    # 1. Receive the 'question'.
    # 2. Use your retriever component to find relevant 'contexts' (list of strings).
    # 3. Use your generator component (LLM) with the question and contexts to produce an 'answer' (string).
    # 4. Return the required dictionary.

    # Example placeholder return value (REPLACE THIS!):
    example_answer = f"This is a placeholder answer generated for the question: '{question}'."
    example_contexts = [
        f"Placeholder context relevant to '{question}'. Context 1.",
        f"Another placeholder context for '{question}'. Context 2.",
        "A generic piece of context that might or might not be relevant.",
    ]

    return {
        "answer": example_answer,
        "contexts": example_contexts,
    }


# --- Step 3: Create a Simple Test Dataset ---
# Define a small set of questions and their ideal answers (ground truths).
def create_tiny_testset() -> List[Dict[str, str]]:
    """
    Creates a small dataset for evaluation.
    'ground_truth' is the ideal answer used for metrics like context_recall.

    Returns:
        List of dictionaries, each with 'question' and 'ground_truth'.
    """
    print("Creating a small test dataset...")
    test_cases = [
        {
            "question": "What is Retrieval-Augmented Generation (RAG)?",
            "ground_truth": "Retrieval-Augmented Generation (RAG) is an AI framework for improving Large Language Model (LLM) responses by grounding them in external knowledge sources. It combines a retrieval system, which fetches relevant information, with a generation system (the LLM), which uses that information to craft the final answer.",
        },
        {
            "question": "How does the Ragas library help?",
            "ground_truth": "Ragas is a framework specifically designed for evaluating RAG pipelines. It provides objective metrics to measure different aspects like the faithfulness of the answer to the context, the relevance of the answer to the question, and the quality (precision, recall) of the retrieved context.",
        },
        {
            "question": "What is the capital of France?",
            "ground_truth": "The capital of France is Paris.",
        }
    ]
    return test_cases


# --- Step 4: Prepare Data for Ragas Evaluation ---
# Run your RAG system on the test questions and format the results for Ragas.
def prepare_evaluation_data(test_cases: List[Dict[str, str]]) -> Dataset:
    print("Preparing data for Ragas evaluation...")

    df = pd.read_csv("./datasets/qa-1300.csv")

    print(df)

    dataset = Dataset.from_pandas(df)

    questions = []
    answers = []
    contexts_list = []
    ground_truths = []

    for test_case in test_cases:
        question = test_case["question"]
        ground_truth = test_case["ground_truth"]
        print(f"\nProcessing question: {question}")

        # Call YOUR RAG system
        rag_output = my_rag_system(question)
        answer = rag_output["answer"]
        contexts = rag_output["contexts"]

        # Append results to lists
        questions.append(question)
        answers.append(answer)
        contexts_list.append(contexts) # contexts should be List[str]
        ground_truths.append(ground_truth)

        print(f"  - Ground Truth: {ground_truth}")
        print(f"  - RAG Answer: {answer}")
        print(f"  - RAG Contexts: {contexts}")

    # Create a dictionary suitable for datasets.Dataset
    data_dict = {
        "question": questions,
        "answer": answers,
        "contexts": contexts_list,
        "ground_truth": ground_truths, # Ragas expects this column name for ground truth answers
    }

    # Convert to Hugging Face Dataset object
    evaluation_dataset = Dataset.from_dict(data_dict)
    print(f"\nData preparation complete. Created Dataset with {len(evaluation_dataset)} examples.")
    return evaluation_dataset


# --- Step 5: Run Ragas Evaluation ---
# Use the Ragas 'evaluate' function with the prepared data and metrics.
def evaluate_rag_with_ragas(dataset: Dataset):
    print("\nStarting Ragas evaluation...")

    # Define the metrics to compute
    metrics = [
        faithfulness,       # Is the answer supported by the context?
        answer_relevancy,   # Is the answer relevant to the question?
        context_precision,  # Are the retrieved contexts relevant? (Signal vs Noise)
        context_recall,     # Were all necessary contexts retrieved? (Needs ground_truth)
    ]

    # Perform the evaluation
    # Pass the dataset, metrics, and the configured LLM
    # If you configured embeddings, pass them too: embeddings=ragas_embeddings
    try:
        results = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=llm_for_eval,
            # embeddings=ragas_embeddings, # Uncomment if you configured and need specific embeddings
            raise_exceptions=False # Set to False to see partial results even if some evaluations fail
        )
        print("Ragas evaluation completed.")
        return results
    except Exception as e:
        print(f"\nAn error occurred during Ragas evaluation: {e}")
        print("Please check the error message and ensure:")
        print("  - Your Google API key is valid and has permissions.")
        print("  - The LLM model ('gemini-1.5-flash-latest') is available.")
        print("  - The input data format (Dataset) is correct.")
        print("  - If using embeddings, ensure they are initialized correctly.")
        # Depending on the error, Ragas might sometimes return partial results
        # if raise_exceptions=False was set during the call.
        # Check if 'results' exists or handle the exception appropriately.
        return None # Indicate failure


# --- Main Execution Block ---
def display_results(evaluation_results):
    print("\n--- Evaluation Results ---")
    # 根据结果类型显示评估结果
    if isinstance(evaluation_results, Dataset):
        results_dict = evaluation_results.to_dict()
        # 计算平均分数
        avg_scores = {metric: sum(results_dict[metric]) / len(results_dict[metric])
                      for metric in results_dict
                      if isinstance(results_dict[metric], list) and
                      len(results_dict[metric]) > 0 and
                      isinstance(results_dict[metric][0], (int, float))}
        print("Average Scores:", avg_scores)

        # 使用pandas展示详细结果
        pd.set_option('display.max_colwidth', None)
        pd.set_option('display.width', 1000)
        results_df = evaluation_results.to_pandas()
        print("\n--- Evaluation Results (Pandas DataFrame) ---")
        print(results_df)
    else:
        print(evaluation_results)
        # 也可以尝试转换为DataFrame展示
        try:
            pd.set_option('display.max_colwidth', None)
            pd.set_option('display.width', 1000)
            results_df = pd.DataFrame([evaluation_results])
            print("\n--- Evaluation Results (Pandas DataFrame) ---")
            print(results_df)
        except Exception as e:
            print(f"\nError converting results to Pandas DataFrame: {e}")


if __name__ == "__main__":
    print("--- RAG System Evaluation Script ---")

    # 1. 创建测试数据集
    test_data = create_tiny_testset()

    # 2. 准备评估数据
    ragas_input_dataset = prepare_evaluation_data(test_data)
    if not ragas_input_dataset:
        print("\nData preparation failed, skipping evaluation.")
        print("\n--- Script execution finished ---")
        exit()

    # 3. 运行Ragas评估
    evaluation_results = evaluate_rag_with_ragas(ragas_input_dataset)
    if evaluation_results:
        display_results(evaluation_results)
    else:
        print("\nEvaluation could not be completed due to errors.")

    print("\n--- Script execution finished ---")